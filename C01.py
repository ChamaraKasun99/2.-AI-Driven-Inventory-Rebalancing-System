# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cn5wWd1fzVx1wMZ-4vh_9sT5rwwTQSlc
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df= pd.read_excel('/content/drive/MyDrive/Projects /2. AI-Driven Inventory Rebalancing System/inventory_data_large_sample.xlsx')
print(df.head()) # Display the first few rows to confirm

# Step 1: Install Required Libraries (Run this once per Colab session)
# Install pandas-profiling (or its successor ydata-profiling)
!pip install ydata-profiling openpyxl plotly --upgrade
# Uninstall and install a compatible numba version if issues persist (often resolved by ydata-profiling)
# !pip uninstall numba -y
# !pip install numba==0.56.4


# Step 2: Import Libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
# Import ProfileReport from ydata_profiling
from ydata_profiling import ProfileReport
from google.colab import drive
import os

# Step 3: Mount Google Drive
drive.mount('/content/drive')

# Step 4: Load Excel Dataset
# Corrected the file path
file_path = '/content/drive/MyDrive/Projects /2. AI-Driven Inventory Rebalancing System/inventory_data_large_sample.xlsx'
df = pd.read_excel(file_path)
df['Date'] = pd.to_datetime(df['Date'])

# Step 5: SKU Trends by Location
sample_skus = df['SKU'].drop_duplicates().sample(3, random_state=42)
sku_trends = df[df['SKU'].isin(sample_skus)]

plt.figure(figsize=(14, 6))
for sku in sample_skus:
    subset = sku_trends[sku_trends['SKU'] == sku]
    # Removed label=sku as hue handles the legend labeling
    sns.lineplot(data=subset, x="Date", y="Demand", hue="Warehouse")
plt.title('SKU Trends by Location')
plt.xlabel('Date')
plt.ylabel('Demand')
plt.legend(title='SKU') # Keep the legend title as SKU
plt.tight_layout()
plt.savefig('/content/drive/MyDrive/EDA_SKU_Trends.png')
plt.show()

# Step 6: Monthly Demand Volatility
df['Month'] = df['Date'].dt.to_period('M')
monthly_volatility = df.groupby(['Month', 'Warehouse'])['Demand'].std().reset_index()
monthly_volatility['Month'] = monthly_volatility['Month'].astype(str)

fig1 = px.line(monthly_volatility, x='Month', y='Demand', color='Warehouse',
               title='Monthly Demand Volatility by Warehouse (Std Dev)')
# Removed fig1.write_image() as it requires a browser environment not available in Colab
fig1.show()

# Step 7: Warehouse-Level Bottlenecks (Inventory < 100)
inventory_threshold = 100
bottlenecks = df[df['Inventory'] < inventory_threshold]
bottleneck_counts = bottlenecks.groupby('Warehouse').size().reset_index(name='Low_Inventory_Count')

plt.figure(figsize=(10, 5))
sns.barplot(data=bottleneck_counts, x='Warehouse', y='Low_Inventory_Count', palette='Reds')
plt.title('Warehouse-Level Bottlenecks (Inventory < 100)')
plt.ylabel('Low Inventory Days')
plt.tight_layout()
plt.savefig('/content/drive/MyDrive/EDA_Bottlenecks.png')
plt.show()

# Step 8: Automated Profiling Report
profile = ProfileReport(df.sample(10000, random_state=42), title="Inventory Data Profile", explorative=True)
profile.to_file("/content/drive/MyDrive/EDA_Inventory_Profile.html")

!pip install prophet openpyxl scikit-learn

import pandas as pd
import numpy as np
from prophet import Prophet
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error
from google.colab import drive
import os
import matplotlib.pyplot as plt

# Get unique combinations
sku_warehouses = df[['SKU', 'Warehouse']].drop_duplicates()

forecast_results = []
forecast_output = []

# Forecasting Horizon
future_days = 30

for _, row in sku_warehouses.iterrows():
    sku = row['SKU']
    warehouse = row['Warehouse']

    data = df[(df['SKU'] == sku) & (df['Warehouse'] == warehouse)][['Date', 'Demand']].rename(columns={'Date': 'ds', 'Demand': 'y'}).sort_values('ds')

    if len(data) < 60:  # Skip if not enough data
        continue

    model = Prophet()
    model.fit(data)

    future = model.make_future_dataframe(periods=future_days)
    forecast = model.predict(future)

    # Save forecasts
    forecast['SKU'] = sku
    forecast['Warehouse'] = warehouse
    forecast_output.append(forecast[['ds', 'yhat', 'SKU', 'Warehouse']])

    # Forecast evaluation on training set
    merged = pd.merge(data, forecast[['ds', 'yhat']], on='ds', how='left')
    y_true = merged['y']
    y_pred = merged['yhat']

    mape = mean_absolute_percentage_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))

    forecast_results.append({
        'SKU': sku,
        'Warehouse': warehouse,
        'MAPE': round(mape * 100, 2),
        'RMSE': round(rmse, 2)
    })

# Export all forecasts and metrics
all_forecasts = pd.concat(forecast_output)
metrics_df = pd.DataFrame(forecast_results)

forecast_csv = '/content/drive/MyDrive/Forecasts/prophet_forecasts.csv'
metrics_csv = '/content/drive/MyDrive/Forecasts/forecast_metrics.csv'

os.makedirs(os.path.dirname(forecast_csv), exist_ok=True)
all_forecasts.to_csv(forecast_csv, index=False)
metrics_df.to_csv(metrics_csv, index=False)

!pip install prophet
import pandas as pd
from prophet import Prophet

df['Date'] = pd.to_datetime(df['Date'])

# Filter SKU and Warehouse
filtered_df = df[(df['SKU'] == 'SKU0096') & (df['Warehouse'] == 'W01')]

# Aggregate daily demand
daily_demand = filtered_df.groupby('Date')['Demand'].sum().reset_index()
daily_demand.columns = ['ds', 'y']

model = Prophet(daily_seasonality=True)
model.fit(daily_demand)

# Create future dataframe
future = model.make_future_dataframe(periods=30)
forecast = model.predict(future)

# Plot forecast
model.plot(forecast);

from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error
import numpy as np

# Merge actual and predicted
merged = pd.merge(daily_demand, forecast[['ds', 'yhat']], on='ds', how='left')

# Calculate metrics
mape = mean_absolute_percentage_error(merged['y'], merged['yhat'])
rmse = np.sqrt(mean_squared_error(merged['y'], merged['yhat']))

print(f'MAPE: {mape:.2%}')
print(f'RMSE: {rmse:.2f}')

forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].to_csv('/content/forecast_SKU0096_W01.csv', index=False)

"""Full Automated Prophet Forecasting Pipeline"""

!pip install prophet
import pandas as pd
from prophet import Prophet
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error
import numpy as np
import os

# Load data
df['Date'] = pd.to_datetime(df['Date'])

# Get unique combinations
sku_warehouses = df[['SKU', 'Warehouse']].drop_duplicates()

# Create output folder
os.makedirs('/content/forecasts', exist_ok=True)

# Store summary metrics
results = []

for idx, row in sku_warehouses.iterrows():
    sku = row['SKU']
    wh = row['Warehouse']

    # Filter data
    temp_df = df[(df['SKU'] == sku) & (df['Warehouse'] == wh)]
    temp_df = temp_df.groupby('Date')['Demand'].sum().reset_index()
    temp_df.columns = ['ds', 'y']

    # Skip if not enough data
    if len(temp_df) < 50:
        continue

    try:
        model = Prophet(daily_seasonality=True)
        model.fit(temp_df)

        # Forecast 30 days ahead
        future = model.make_future_dataframe(periods=30)
        forecast = model.predict(future)

        # Merge and evaluate
        merged = pd.merge(temp_df, forecast[['ds', 'yhat']], on='ds', how='left')
        mape = mean_absolute_percentage_error(merged['y'], merged['yhat'])
        rmse = np.sqrt(mean_squared_error(merged['y'], merged['yhat']))

        # Save forecast
        fname = f'/content/forecasts/forecast_{sku}_{wh}.csv'
        forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].to_csv(fname, index=False)

        # Save metrics
        results.append({'SKU': sku, 'Warehouse': wh, 'MAPE': mape, 'RMSE': rmse})

    except Exception as e:
        print(f'Error processing {sku}-{wh}: {e}')

# Save summary to CSV
metrics_df = pd.DataFrame(results)
metrics_df.to_csv('/content/forecasts/forecast_accuracy_summary.csv', index=False)

# Display top accurate forecasts
metrics_df.sort_values('MAPE').head(10)

"""Phase 5: Segmentation with KMeans"""

# Aggregate features
sku_stats = df.groupby('SKU').agg({
    'Demand': ['sum', 'mean', 'std']
}).reset_index()
sku_stats.columns = ['SKU', 'Total_Demand', 'Avg_Daily_Demand', 'Demand_STD']

warehouse_stats = df.groupby('Warehouse').agg({
    'Demand': ['sum', 'mean', 'std'],
    'SKU': pd.Series.nunique
}).reset_index()
warehouse_stats.columns = ['Warehouse', 'Total_Demand', 'Avg_Demand', 'Demand_STD', 'Unique_SKUs']

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

sku_features = sku_stats[['Total_Demand', 'Avg_Daily_Demand', 'Demand_STD']]
sku_scaled = StandardScaler().fit_transform(sku_features)

kmeans_sku = KMeans(n_clusters=3, random_state=42)
sku_stats['SKU_Cluster'] = kmeans_sku.fit_predict(sku_scaled)

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Perform KMeans clustering for warehouses
warehouse_features = warehouse_stats[['Total_Demand', 'Avg_Demand', 'Demand_STD', 'Unique_SKUs']]
warehouse_scaled = StandardScaler().fit_transform(warehouse_features)

kmeans_warehouse = KMeans(n_clusters=3, random_state=42) # Using 3 clusters as an example, adjust as needed
warehouse_stats['Warehouse_Cluster'] = kmeans_warehouse.fit_predict(warehouse_scaled)

sns.set(style="whitegrid")
plt.figure(figsize=(8,6))
sns.scatterplot(data=sku_stats, x='Avg_Daily_Demand', y='Demand_STD', hue='SKU_Cluster', palette='Set2')
plt.title("SKU Clustering by Avg Demand vs Std Dev")
plt.savefig("/content/forecasts/sku_segmentation_plot.png")
plt.show()

plt.figure(figsize=(8,6))
sns.scatterplot(data=warehouse_stats, x='Unique_SKUs', y='Avg_Demand', hue='Warehouse_Cluster', palette='Set1')
plt.title("Warehouse Clustering")
plt.savefig("/content/forecasts/warehouse_segmentation_plot.png")
plt.show()

sku_stats.to_csv('/content/forecasts/sku_segmentation_labels.csv', index=False)
warehouse_stats.to_csv('/content/forecasts/warehouse_segmentation_labels.csv', index=False)

"""Phase 6: Inventory Rebalancing Optimization (PuLP)"""

!pip install pulp
import pandas as pd
from pulp import LpProblem, LpMinimize, LpVariable, lpSum, LpStatus

# Simplify: Use total forecasted demand over next 30 days (or a relevant horizon)
# Assuming 'all_forecasts' DataFrame from the forecasting step is available and contains 'ds', 'yhat', 'SKU', 'Warehouse'
# Filter for future dates (adjust based on your forecast horizon, e.g., next 14 or 30 days)
# For this example, let's consider the entire forecast period available in all_forecasts
future_demand = all_forecasts.groupby(['SKU', 'Warehouse'])['yhat'].sum().reset_index()
future_demand.columns = ['SKU', 'Warehouse', 'Forecasted_Demand']

# Current stock: Get the latest inventory level for each SKU at each warehouse from the main df
# Assuming df is sorted by Date, we can get the last entry for each group
# If not sorted, you might need to sort or use a more robust method to get the latest date's inventory
latest_inventory = df.sort_values('Date').groupby(['SKU', 'Warehouse'])['Inventory'].last().reset_index()
latest_inventory.columns = ['SKU', 'Warehouse', 'Current_Stock']


# Merge dataframes
# Merge future demand and current stock
demand_stock_df = pd.merge(future_demand, latest_inventory, on=['SKU', 'Warehouse'], how='left')
# Fill potential missing values in Current_Stock with 0 if an SKU/Warehouse combination exists in forecasts but not in historical inventory (unlikely but good practice)
demand_stock_df['Current_Stock'] = demand_stock_df['Current_Stock'].fillna(0)


# Sample transfer cost matrix (can be distance or cost per unit transferred)
warehouses = df['Warehouse'].unique()
# Create a more structured cost matrix DataFrame for easier lookup
transfer_costs = pd.DataFrame([(w1, w2, abs(hash(w1) - hash(w2)) % 10 + 1)
                               for w1 in warehouses for w2 in warehouses if w1 != w2],
                              columns=['Source', 'Destination', 'Cost'])

# Create model
model = LpProblem("Inventory_Rebalancing", LpMinimize)

# Variables: transfer quantity from source to target for each SKU
transfers = {}
for sku in df['SKU'].unique():
    for w1 in warehouses:
        for w2 in warehouses:
            if w1 != w2:
                var_name = f"transfer_{sku}_{w1}_to_{w2}"
                transfers[(sku, w1, w2)] = LpVariable(var_name, lowBound=0, cat='Continuous')

# Create dictionaries for easier lookup of demand and current stock
demand_dict = demand_stock_df.set_index(['SKU', 'Warehouse'])['Forecasted_Demand'].to_dict()
current_stock_dict = demand_stock_df.set_index(['SKU', 'Warehouse'])['Current_Stock'].to_dict()

# Create a dictionary for easier lookup of transfer costs
cost_dict = transfer_costs.set_index(['Source', 'Destination'])['Cost'].to_dict()


# Objective: Minimize total cost of transfers
model += lpSum([
    transfers[(sku, w1, w2)] * cost_dict.get((w1, w2), 0) # Use cost_dict for lookup, default to 0 if cost is not defined
    for (sku, w1, w2) in transfers
])

# Constraints:
# 1. Ensure no warehouse ships more than its available inventory
for sku in df['SKU'].unique():
    for w1 in warehouses:
        # Use current_stock_dict for available inventory
        available = current_stock_dict.get((sku, w1), 0)
        model += lpSum([transfers[(sku, w1, w2)] for w2 in warehouses if w1 != w2]) <= available, f"Supply_Constraint_{sku}_{w1}"

# 2. Ensure each warehouse meets its forecasted demand after receiving stock
for sku in df['SKU'].unique():
    for w2 in warehouses:
        # Use demand_dict for forecasted demand
        demand = demand_dict.get((sku, w2), 0)
        # Use current_stock_dict for current stock
        current = current_stock_dict.get((sku, w2), 0)
        incoming = lpSum([transfers[(sku, w1, w2)] for w1 in warehouses if w1 != w2])
        outgoing = lpSum([transfers[(sku, w2, w1)] for w1 in warehouses if w1 != w2])
        # Constraint: current stock + incoming transfers - outgoing transfers >= forecasted demand
        model += current + incoming - outgoing >= demand, f"Demand_Constraint_{sku}_{w2}"

model.solve()
print(f"Status: {LpStatus[model.status]}")

# Extract optimal plan
results = []
for (sku, w1, w2), var in transfers.items():
    if var.varValue > 0:
        results.append({
            'SKU': sku,
            'Source': w1,
            'Target': w2,
            'Transfer_Qty': var.varValue,
            'Cost': var.varValue * cost_dict.get((w1, w2), 0) # Use cost_dict to get the transfer cost
        })

opt_plan = pd.DataFrame(results)
opt_plan.to_csv('/content/forecasts/optimal_inventory_transfer_plan.csv', index=False)

# Cost summary
total_cost = opt_plan['Cost'].sum()
print(f"✅ Total Transfer Cost: {total_cost:.2f}")