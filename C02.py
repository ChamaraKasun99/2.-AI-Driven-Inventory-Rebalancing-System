# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ElmgsrUiMgzG91TUQydQMRpMShVgaq2D
"""

# üì¶ AI-Driven Inventory Rebalancing System ‚Äì Full Pipeline (Google Colab Version)

# ‚úÖ Setup
!pip install prophet pulp scikit-learn matplotlib seaborn openpyxl --quiet

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from prophet import Prophet
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from pulp import LpProblem, LpMinimize, LpVariable, lpSum, LpStatus
from google.colab import drive
import os

# üîó Mount Google Drive (Optional - not needed for uploaded file)
# drive.mount('/content/drive')
save_path = "/content/drive/MyDrive/Projects /2. AI-Driven Inventory Rebalancing System" # Save outputs in the current session
os.makedirs(save_path, exist_ok=True)

# üì• Load Dataset
# Use the path for the uploaded file
file_path = "/content/drive/MyDrive/Projects /2. AI-Driven Inventory Rebalancing System/inventory_data_large_sample.xlsx"
df = pd.read_excel(file_path)
df['Date'] = pd.to_datetime(df['Date'])

# üìä Phase 3: EDA
sns.set(style='whitegrid')
plt.figure(figsize=(10,6))
sns.lineplot(data=df, x='Date', y='Demand', hue='Warehouse')
plt.title("Warehouse Demand Trends")
plt.savefig(f"{save_path}/warehouse_trends.png")
plt.close()

# üìà Phase 4: Forecasting
forecast_list = []
forecast_results = []

for (sku, wh), group in df.groupby(['SKU', 'Warehouse']):
    # Ensure the group has enough data points and correct columns for Prophet
    if len(group) < 30 or 'Date' not in group.columns or 'Demand' not in group.columns:
        print(f"Skipping forecasting for SKU: {sku}, Warehouse: {wh} due to insufficient or missing data.")
        continue

    # Prepare data for Prophet
    prophet_df = group.groupby('Date').agg({'Demand': 'sum'}).reset_index()
    prophet_df.rename(columns={'Date': 'ds', 'Demand': 'y'}, inplace=True)

    # Ensure 'ds' is datetime and 'y' is numeric
    prophet_df['ds'] = pd.to_datetime(prophet_df['ds'])
    prophet_df['y'] = pd.to_numeric(prophet_df['y'])

    # Drop rows with NaN or NaT values that might cause issues
    prophet_df.dropna(subset=['ds', 'y'], inplace=True)

    # Check again after processing if there is still enough data
    if len(prophet_df) < 30:
         print(f"Skipping forecasting for SKU: {sku}, Warehouse: {wh} after data cleaning due to insufficient data.")
         continue

    try:
        model = Prophet()
        model.fit(prophet_df)
        future = model.make_future_dataframe(periods=30)
        forecast = model.predict(future)
        forecast['SKU'] = sku
        forecast['Warehouse'] = wh
        forecast_list.append(forecast[['ds', 'yhat', 'SKU', 'Warehouse']])
    except Exception as e:
        print(f"Error forecasting for SKU: {sku}, Warehouse: {wh} - {e}")
        continue


if forecast_list:
    forecast_df = pd.concat(forecast_list)
    forecast_df.to_csv(f"{save_path}/forecast_results.csv", index=False)
else:
    forecast_df = pd.DataFrame(columns=['ds', 'yhat', 'SKU', 'Warehouse'])
    print("No sufficient data to generate forecasts.")


# üìâ Accuracy Summary (MAPE Sample)
from sklearn.metrics import mean_absolute_percentage_error
# Check if sample data exists before calculating MAPE
sample = df[(df['SKU'] == 'SKU0096') & (df['Warehouse'] == 'W01')].groupby('Date')['Demand'].sum()
if not sample.empty:
    sample_df = sample.reset_index()
    sample_df.columns = ['ds', 'y']
    # Ensure 'ds' is datetime and 'y' is numeric for MAPE sample
    sample_df['ds'] = pd.to_datetime(sample_df['ds'])
    sample_df['y'] = pd.to_numeric(sample_df['y'])
    sample_df.dropna(subset=['ds', 'y'], inplace=True)

    if len(sample_df) >= 2: # Need at least 2 points to calculate MAPE
      try:
          m = Prophet()
          m.fit(sample_df)
          # Predict only on the training data for MAPE calculation
          predictions = m.predict(sample_df.drop(columns='y'))
          mape = mean_absolute_percentage_error(sample_df['y'], predictions['yhat'])
          print("Sample MAPE:", round(mape, 3))
      except Exception as e:
          print(f"Error calculating Sample MAPE: {e}")
    else:
        print("Skipping Sample MAPE calculation: Insufficient data after cleaning for SKU0096 and W01.")

else:
    print("Skipping Sample MAPE calculation: No data for SKU0096 and W01.")


# üß† Phase 5: Clustering
sku_stats = df.groupby('SKU').agg({'Demand': ['sum', 'mean', 'std']}).reset_index()
sku_stats.columns = ['SKU', 'Total_Demand', 'Avg_Demand', 'Demand_STD']

# Check if sku_stats is empty before clustering
if not sku_stats.empty and len(sku_stats) >= KMeans(n_clusters=3, n_init=10).n_clusters:
    # Drop rows with NaN or inf values that might cause issues with scaling
    sku_stats.replace([np.inf, -np.inf], np.nan, inplace=True)
    sku_stats.dropna(subset=['Total_Demand', 'Avg_Demand', 'Demand_STD'], inplace=True)

    if len(sku_stats) >= KMeans(n_clusters=3, n_init=10).n_clusters: # Check again after dropping NaNs
        X_sku = StandardScaler().fit_transform(sku_stats[['Total_Demand', 'Avg_Demand', 'Demand_STD']])
        kmeans_sku = KMeans(n_clusters=3, random_state=42, n_init=10).fit(X_sku) # Add n_init
        sku_stats['Cluster'] = kmeans_sku.labels_
        sku_stats.to_csv(f"{save_path}/sku_clusters.csv", index=False)
        print("SKU clustering completed.")
    else:
        print("Skipping SKU clustering: Insufficient data after cleaning for clustering.")
        sku_stats['Cluster'] = -1 # Assign a default cluster or handle as needed

else:
    print("Skipping SKU clustering: Insufficient data for clustering.")
    sku_stats['Cluster'] = -1 # Assign a default cluster or handle as needed


# üßÆ Phase 6: Optimization
# Create dummy current stock
stock = df.groupby(['SKU', 'Warehouse'])['Demand'].sum().reset_index()
stock.columns = ['SKU', 'Warehouse', 'Current_Stock']

# Join forecast
# Ensure forecast_df is not empty before merging
if not forecast_df.empty:
    latest_forecast = forecast_df.groupby(['SKU', 'Warehouse']).agg({'yhat': 'sum'}).reset_index()
    latest_forecast.columns = ['SKU', 'Warehouse', 'Forecast_Demand']
    data = pd.merge(stock, latest_forecast, on=['SKU', 'Warehouse'], how='inner')
    data['Transferable'] = data['Current_Stock'] - data['Forecast_Demand']
    data = data[data['Transferable'] > 0].copy() # Use .copy() to avoid SettingWithCopyWarning

    # Define Optimization
    prob = LpProblem("Inventory_Rebalancing", LpMinimize)
    vars = {}
    warehouses = df['Warehouse'].unique()

    if not data.empty and warehouses.size > 0:
        for i, row in data.iterrows():
            for wh_target in warehouses:
                if wh_target != row['Warehouse']:
                    key = (row['SKU'], row['Warehouse'], wh_target)
                    vars[key] = LpVariable(f"Transfer_{key}", lowBound=0)

        # Cost: Assume unit transfer cost = 1
        prob += lpSum([vars[k]*1 for k in vars])

        # Constraints: Ensure transferred does not exceed surplus
        for (sku, wh_from, wh_to), var in vars.items():
             # Ensure the key exists in the data DataFrame before accessing
            filtered_data = data[(data['SKU'] == sku) & (data['Warehouse'] == wh_from)]
            if not filtered_data.empty:
                transferable = filtered_data['Transferable'].values[0]
                prob += var <= transferable
            else:
                 # If the key is not in data, the transferable amount is 0, constraint is 0 <= var, which is handled by lowBound=0
                 pass


        # Solve
        # Check if there are variables to solve for
        if vars:
            prob.solve()
            print("Optimization Status:", LpStatus[prob.status])

            # Results
            transfer_plan = []
            for (sku, src, tgt), var in vars.items():
                # Check if var.varValue is not None and greater than 0
                if var.varValue is not None and var.varValue > 0:
                    transfer_plan.append([sku, src, tgt, var.varValue])

            transfer_df = pd.DataFrame(transfer_plan, columns=['SKU', 'Source', 'Target', 'Quantity'])
            transfer_df.to_csv(f"{save_path}/transfer_plan.csv", index=False)
            print("Transfer Plan saved!")
        else:
            print("Skipping Optimization: No variables to optimize.")

    else:
        print("Skipping Optimization: Insufficient data for optimization after merging forecast.")
else:
    print("Skipping Optimization: Forecast data is empty.")

# üì¶ AI-driven Inventory Rebalancing System with Simple UI
# Run in Google Colab

!pip install prophet pulp openpyxl --quiet

from IPython.display import display, clear_output
import ipywidgets as widgets
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from prophet import Prophet
from sklearn.cluster import KMeans
from pulp import LpProblem, LpMinimize, LpVariable, lpSum
import numpy as np

# Define interface
upload = widgets.FileUpload(accept='.xlsx', multiple=False)
run_button = widgets.Button(description="Run Pipeline", button_style='success')
output = widgets.Output()

title = widgets.HTML("<h2>üì¶ AI-driven Inventory Rebalancing System</h2>")

def run_pipeline(b):
    with output:
        clear_output()
        if len(upload.value) == 0:
            print("‚ùå Please upload an Excel (.xlsx) file.")
            return

        # Save uploaded file
        file_data = list(upload.value.values())[0]
        with open("inventory_data.xlsx", "wb") as f:
            f.write(file_data['content'])

        print("‚úÖ File uploaded. Processing...")
        df = pd.read_excel("inventory_data.xlsx")

        # Sample trend
        print("\nüîç Exploratory Data Analysis...")
        sample_sku = df['SKU'].unique()[0]
        sample_df = df[df['SKU'] == sample_sku].groupby('Date')['Demand'].sum().reset_index()
        plt.figure(figsize=(10, 4))
        plt.plot(sample_df['Date'], sample_df['Demand'], marker='o')
        plt.title(f'Demand Trend for {sample_sku}')
        plt.grid(True)
        plt.show()

        # Forecasting
        print("\nüìà Forecasting Demand with Prophet...")
        sample_combo = df[(df['SKU'] == sample_sku) & (df['Warehouse'] == df['Warehouse'].unique()[0])]
        prophet_df = sample_combo.groupby('Date')['Demand'].sum().reset_index()
        prophet_df.columns = ['ds', 'y']
        m = Prophet()
        m.fit(prophet_df)
        future = m.make_future_dataframe(periods=7)
        forecast = m.predict(future)
        m.plot(forecast)
        plt.title("Forecast (Next 7 Days)")
        plt.show()

        # Clustering
        print("\nüî¢ Clustering SKUs by Volume...")
        sku_stats = df.groupby('SKU')['Demand'].sum().reset_index()
        kmeans = KMeans(n_clusters=2, random_state=0).fit(sku_stats[['Demand']])
        sku_stats['Cluster'] = kmeans.labels_
        sns.barplot(data=sku_stats, x='SKU', y='Demand', hue='Cluster')
        plt.title("SKU Clusters")
        plt.xticks(rotation=90)
        plt.show()

        # Optimization
        print("\n‚öôÔ∏è Inventory Rebalancing Optimization...")
        inventory = df.groupby(['Warehouse', 'SKU']).agg({'Inventory': 'sum', 'Demand': 'sum'}).reset_index()
        warehouses = df['Warehouse'].unique()
        skus = df['SKU'].unique()

        model = LpProblem("Inventory_Rebalancing", LpMinimize)
        transfer = LpVariable.dicts("Transfer",
                                    [(w1, w2, sku) for w1 in warehouses for w2 in warehouses if w1 != w2 for sku in skus],
                                    lowBound=0)

        model += lpSum([transfer[w1, w2, sku] for (w1, w2, sku) in transfer])

        for (w1, sku) in inventory.groupby(['Warehouse', 'SKU']).sum().index:
            available = inventory[(inventory['Warehouse'] == w1) & (inventory['SKU'] == sku)]['Inventory'].values[0]
            model += lpSum([transfer[w1, w2, sku] for w2 in warehouses if w1 != w2]) <= available

        model.solve()
        print(f"üü¢ Optimization Done: Status {model.status}, Total Transfers: {model.objective.value()}")

        print("\nüöö Suggested Transfers:")
        count = 0
        for (w1, w2, sku), var in transfer.items():
            if var.varValue and var.varValue > 0:
                print(f"Transfer {int(var.varValue)} of {sku} from {w1} ‚Üí {w2}")
                count += 1
                if count > 10: break

run_button.on_click(run_pipeline)
display(title, upload, run_button, output)